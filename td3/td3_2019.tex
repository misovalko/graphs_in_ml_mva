\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm,algorithmic,caption} % lecture 5
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{listings}
\setenumerate[1]{label=\thesection.\arabic*.}
\usepackage{datetime}
\include{mathdef}


\setlength{\parskip}{0.5\baselineskip}%
\setlength\parindent{0pt}


\begin{document}
	\title{TP3: Graph Neural Networks}
	\author{omar (dot) darwiche-domingues (at) inria.fr \\
			pierre (dot) perrault (at) inria.fr}
	\date{December 6, 2019}
	\maketitle

	\begin{abstract}
		The report and the code are due in 2 weeks (deadline 23:59, 20/12/2019).
		You will find instructions on how to submit the report on piazza,
		as well as the policies for scoring and late submissions.
	\end{abstract}

\section{Neural Relational Inference}

This practical session is based on the paper \href{https://arxiv.org/pdf/1802.04687.pdf}{\emph{Neural Relational Inference for Interacting Systems}} by Kipf et al., 2018.

We will use the following material provided by Marc Lelarge and Timoth\'ee Lacroix: \url{https://github.com/timlacroix/nri_practical_session}. 

\subsection{Motivation and problem formulation}
A wide range of dynamical systems can be seen as a group of interacting components. For example, we can think of a set of 2-dimensional particles coupled by springs. Assume that we are given only a set of trajectories of such interacting dynamical system. How can we learn its dynamical model in an unsupervised way?

Formally, we are given as input a set of trajectories of $N$ objects, and each trajectory has length $T$. Each object $i$, for $i=1,\ldots, N$, is represented by a vertex $v_i$. Let $\mathbf{x}_i^t$ be the feature vector of object $i$ at time $t$ (e.g., position and velocity) with dimension $D$. Let $\mathbf{x}^t = \{\mathbf{x}_1^t, \ldots, \mathbf{x}_N^t\}$ be the set of features of all $N$ objects at time $t$ and let $\mathbf{x}_i = (\mathbf{x}_i^1, \ldots, \mathbf{x}_i^T)$ be the trajectory of object $i$. The input data can be stored in a 3-dimensional array $\mathbf{x}$ of shape $N\times T \times D$, denoted by $\mathbf{x} = (\mathbf{x}^1, \ldots, \mathbf{x}^T)$, such that $\mathbf{x}_{i,t,d}$ is the $d$-th component of the feature vector of object $i$ at time $t$. 

In addition,  we assume that the dynamics can be modeled by a graph neural network (GNN) given an unknown graph $\mathbf{z}$ where $\mathbf{z}_{i,j}$ represents the discrete edge type between objects $v_i$ and $v_j$.

In this context, we want to learn, simultaneously:

\begin{itemize}
	\item The edge types $\mathbf{z}_{i,j}$ (\textbf{edge type estimation}); 
	\item A model that, for any time $t$, takes $\mathbf{x}^t$ as input and predicts $\mathbf{x}^{t+1}$ as output (\textbf{future state prediction}).
\end{itemize}

\subsection{Model}

The Neural Relational Inference (NRI) model consists of:

\begin{itemize}
	\item An \textbf{encoder} that uses trajectories $\mathbf{x} = (\mathbf{x}^1, \ldots, \mathbf{x}^T)$ to infer pairwise interaction vectors $\mathbf{z}_{i, j} \in \mathbb{R}^K$ for $i, j$ in $\{1, \ldots, N \}$, where $K$ is the number of \emph{edge types}.
	\item A \textbf{decoder} that takes $\mathbf{x}^t$ and $\mathbf{z} = \{\mathbf{z}_{i, j}\}_{i,j}$ as input to infer $\mathbf{x}^{t+1}$.
\end{itemize}

Both the encoder and the decoder are implemented using graph neural networks. For more details, read Section 3 of the paper \href{https://arxiv.org/pdf/1802.04687.pdf}{here}.

 
\section{Questions}

Complete the code in the following notebook

\url{https://github.com/timlacroix/nri_practical_session/blob/master/NRI_student.ipynb}

and answer the questions below in your report. \textbf{For the report, no code submission is required}. Note that this Github repository contains a \texttt{solutions} folder, which you are allowed to use to complete the notebook. 

\begin{enumerate}
	\item Explain what are the edge types $\mathbf{z}_{i,j}$.
	\item Explain how the encoder and the decoder work.
	\item Explain the LSTM baseline used for joint trajectory prediction. Why is it important to have a ``burn-in'' phase?
	\item Consider the training of the LSTM baseline. Notice that the negative log-likelihood is lower after the burn-in than before. Why is this surprising? Why is this happening?
	\item Consider the problem of trajectory prediction. What are the advantages of the NRI model with respect to the LSTM baseline?
	\item Consider the training the of NRI model. What do you notice about the edge accuracy during training? Why is this surprising? 
	\item What do you expect to happen with the NRI model when there is no interaction between the objects?
\end{enumerate}

\end{document}